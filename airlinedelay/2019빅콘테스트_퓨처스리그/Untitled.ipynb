{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨링\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    " \n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(data['FLO'])\n",
    "data['FLO'] = encoder.transform(data['FLO'])\n",
    "encoder.fit(data['FLT'])\n",
    "data['FLT'] = encoder.transform(data['FLT'])\n",
    "encoder.fit(data['SDT_DY'])\n",
    "data['SDT_DY'] = encoder.transform(data['SDT_DY'])\n",
    "encoder.fit(data['AOD'])\n",
    "data['AOD'] = encoder.transform(data['AOD'])\n",
    "encoder.fit(data['IRR'])\n",
    "data['IRR'] = encoder.transform(data['IRR'])\n",
    "encoder.fit(data['DLY'])\n",
    "data['DLY'] = encoder.transform(data['DLY'])\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# 학습\n",
    "\n",
    "y= data['DLY']\n",
    "X =data.drop('DLY', axis= 1)\n",
    "X\n",
    "\n",
    "from imblearn.over_sampling import ADASYN \n",
    "sm = ADASYN()\n",
    "X, y = sm.fit_resample(X, y)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.loc[:, data.columns != 'DLY'], data['DLY'], random_state=0)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "#-------------------------------------------\n",
    "\n",
    "# PDP\n",
    "\n",
    "# Partial Dependence Plots - adapted from Dan B NB on Kaggle()\n",
    "import pandas as pd\n",
    "from pandas import read_csv, DataFrame\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n",
    "\n",
    "# get_some_data is defined in hidden cell above.\n",
    "# scikit-learn originally implemented partial dependence plots only for Gradient Boosting models\n",
    "# this was due to an implementation detail, and a future release will support all model types.\n",
    "my_model = GradientBoostingRegressor()\n",
    "# fit the model as usual\n",
    "temp_x = X[:]\n",
    "temp_x.drop('STTATT', axis=1)\n",
    "my_model.fit(temp_x, y)\n",
    "# Here we make the plot\n",
    "\n",
    "temp_x\n",
    "\n",
    "temp_x.columns.values.tolist()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,30)\n",
    "my_plots = plot_partial_dependence(my_model,\n",
    "                                   features=[i for i in range(12,len(temp_x.columns.values.tolist()))],\n",
    "                                   feature_names=temp_x.columns.values.tolist(), # column numbers of plots we want to show\n",
    "                                   X=temp_x,            # raw predictors data.\n",
    "                                   grid_resolution=100000) # number of values to plot on x axis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
